{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune BART-IT on summarization datasets\n",
    "\n",
    "- ARTeLab/fanpage\n",
    "- **ARTeLab/ilpost (questo notebook)**\n",
    "- ARTeLab/mlsum-it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARTeLab/ilpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export COMET_API_KEY=0bfHTco7gsezAkISIiqrE0OPp\n",
    "! export COMET_PROJECT_NAME=bart-it-ilpost\n",
    "! export COMET_WORKSPACE=morenolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlaquatra/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "import transformers\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"bart-it-s/\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tokenizer_bart_it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ARTeLab--ilpost-92e20cc6892f0877\n",
      "Found cached dataset csv (/home/mlaquatra/.cache/huggingface/datasets/ARTeLab___csv/ARTeLab--ilpost-92e20cc6892f0877/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 966.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train:  35201\n",
      "Len test:  4400\n",
      "Len val:  4400\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ARTeLab/ilpost\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "print (\"Len train: \", len(train_dataset))\n",
    "print (\"Len test: \", len(test_dataset))\n",
    "print (\"Len val: \", len(val_dataset))\n",
    "\n",
    "\n",
    "# average length input in training set\n",
    "avg_len_train = sum([len(tokenizer.encode(x)) for x in train_dataset[\"source\"]]) / len(train_dataset)\n",
    "# average length output in training set\n",
    "avg_len_output_train = sum([len(tokenizer.encode(x)) for x in train_dataset[\"target\"]]) / len(train_dataset)\n",
    "\n",
    "# average length input in test set\n",
    "avg_len_test = sum([len(tokenizer.encode(x)) for x in test_dataset[\"source\"]]) / len(test_dataset)\n",
    "# average length output in test set\n",
    "avg_len_output_test = sum([len(tokenizer.encode(x)) for x in test_dataset[\"target\"]]) / len(test_dataset)\n",
    "\n",
    "# average length input in validation set\n",
    "avg_len_val = sum([len(tokenizer.encode(x)) for x in val_dataset[\"source\"]]) / len(val_dataset)\n",
    "# average length output in validation set\n",
    "avg_len_output_val = sum([len(tokenizer.encode(x)) for x in val_dataset[\"target\"]]) / len(val_dataset)\n",
    "\n",
    "\n",
    "\n",
    "train_input = train_dataset[\"source\"]\n",
    "train_target = train_dataset[\"target\"]\n",
    "\n",
    "test_input = test_dataset[\"source\"]\n",
    "test_target = test_dataset[\"target\"]\n",
    "\n",
    "val_input = val_dataset[\"source\"]\n",
    "val_target = val_dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train: 35201 - Avg len input: 241.30766171415584 - Avg len output: 38.918922757876196\n",
      "Len test: 4400 - Avg len input: 242.61704545454546 - Avg len output: 38.789545454545454\n",
      "Len val: 4400 - Avg len input: 242.61704545454546 - Avg len output: 38.789545454545454\n"
     ]
    }
   ],
   "source": [
    "print (f\"Len train: {len(train_dataset)} - Avg len input: {avg_len_train} - Avg len output: {avg_len_output_train}\")\n",
    "print (f\"Len test: {len(test_dataset)} - Avg len input: {avg_len_test} - Avg len output: {avg_len_output_test}\")\n",
    "print (f\"Len val: {len(val_dataset)} - Avg len input: {avg_len_val} - Avg len output: {avg_len_output_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class is inteded for sequence classification tasks.\n",
    "    :param source_text: List of source text that is used as input to the model.\n",
    "    :param target_text: List of target text that is used as expected output from the model.\n",
    "    :param tokenizer: The tokenizer to be used for tokenizing the texts. It can be an instance of the transformers AutoTokenizer class or a custom tokenizer.\n",
    "    :param max_input_length: The maximum length of the tokenized input text.\n",
    "    :param max_output_length: The maximum length of the tokenized output text.\n",
    "    :param padding: The padding strategy to be used. Available options are available in the transformers library.\n",
    "    :param truncation: Whether to truncate the text or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_text: List[str],\n",
    "        target_text: List[str],\n",
    "        tokenizer,\n",
    "        max_input_length: int = 1024,\n",
    "        max_output_length: int = 128,\n",
    "        padding: str = \"max_length\",\n",
    "        truncation: bool = True,\n",
    "    ):\n",
    "\n",
    "        self.source_text = source_text\n",
    "        self.target_text = target_text\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = padding\n",
    "        self.truncation = truncation\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This function is called to get the tokenized source and target text for a given index.\n",
    "        :param idx: The index of the text and label to be returned.\n",
    "        :return: A dictionary containing the tokenized source (`input_ids`) with attention mask (`attention_mask`) and the tokenized target (`labels`).\n",
    "        \"\"\"\n",
    "        input = self.tokenizer(\n",
    "            self.source_text[idx],\n",
    "            max_length=self.max_input_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        output = self.tokenizer(\n",
    "            text_target = self.target_text[idx],\n",
    "            max_length=self.max_output_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": input[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": output[\"input_ids\"].squeeze(),\n",
    "        }\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_text)\n",
    "\n",
    "train_dataset = Dataset(source_text=train_input, target_text=train_target, tokenizer=tokenizer)\n",
    "test_dataset = Dataset(source_text=test_input, target_text=test_target, tokenizer=tokenizer)\n",
    "val_dataset = Dataset(source_text=val_input, target_text=val_target, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"ilpost_bart_it\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LOG_DIR = \"ilpost_bart_it_logs\"\n",
    "LOGGING_STEPS = 100\n",
    "LEARNING_RATE = 5e-5\n",
    "DATALOADER_NUM_WORKERS = 4\n",
    "SAVE_TOTAL_LIMIT = 5\n",
    "USE_CUDA = True\n",
    "FP16 = True\n",
    "HUB_MODEL_ID = \"\"\n",
    "PUSH_TO_HUB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    dataloader_num_workers=DATALOADER_NUM_WORKERS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    no_cuda=not (USE_CUDA),\n",
    "    fp16=FP16,\n",
    "    metric_for_best_model=\"R2\",\n",
    "    greater_is_better=True,\n",
    "    hub_model_id=HUB_MODEL_ID,\n",
    "    push_to_hub=PUSH_TO_HUB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "/home/mlaquatra/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 35201\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlaquatra/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='419' max='5510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 419/5510 05:16 < 1:04:23, 1.32 it/s, Epoch 0.76/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions[0]\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str,\n",
    "        references=label_str,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"R1\": round(rouge_output[\"rouge1\"], 4),\n",
    "        \"R2\": round(rouge_output[\"rouge2\"], 4),\n",
    "        \"RL\": round(rouge_output[\"rougeL\"], 4),\n",
    "        \"RLsum\": round(rouge_output[\"rougeLsum\"], 4),\n",
    "    }\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 8437\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlaquatra/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{'eval_loss': 0.9436779618263245, 'eval_R1': 0.5396, 'eval_R2': 0.2838, 'eval_RL': 0.5023, 'eval_RLsum': 0.5024, 'eval_runtime': 57.3394, 'eval_samples_per_second': 147.141, 'eval_steps_per_second': 2.302, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print (trainer.evaluate(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'eval_loss': 0.9436779618263245, 'eval_R1': 0.5396, 'eval_R2': 0.2838, 'eval_RL': 0.5023, 'eval_RLsum': 0.5024, 'eval_runtime': 57.3394, 'eval_samples_per_second': 147.141, 'eval_steps_per_second': 2.302, 'epoch': 10.0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52f325c05a492bcb334a2daaac7c5a037fbdd4e22a7d70f803f6eab21ceb4722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
